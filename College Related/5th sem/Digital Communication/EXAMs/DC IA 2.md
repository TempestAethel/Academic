# QUESTION BANK

### 1. Consider a (6,3) linear block code whose generator matrix is given by:
   - (i) Find the parity check matrix.
   - (ii) Find the minimum distance of the code.
   - (iii) Draw the encoder and syndrome computation circuit.
---
### 2. Design a linear block code with a minimum distance of 3 and a message block size of 8 bits.

<p><strong>Designing a Linear Block Code with a Minimum Distance of 3 and Message Block Size of 8 Bits</strong></p>

<p>To design a linear block code with a minimum distance of 3 and a message block size of 8 bits, we need to carefully choose the parameters of the code, including the number of codeword bits, the generator matrix, and the parity-check matrix. The minimum distance \(d_{\min}\) of a code is the smallest number of differing bits between any two distinct codewords. A minimum distance of 3 ensures that the code can detect up to two errors and correct one error in any received codeword.</p>

<h3>Step-by-step Design</h3>

<h4>1. Parameters of the Code</h4>
<ul>
  <li><strong>Message Block Size (k):</strong> The size of the message block is 8 bits, so \( k = 8 \).</li>
  <li><strong>Codeword Length (n):</strong> The length of each codeword is chosen such that the minimum distance is 3. We will choose \( n = 12 \) (as a reasonable choice to satisfy the minimum distance and error correction properties).</li>
  <li><strong>Minimum Distance (d):</strong> The minimum distance of the code is 3, which guarantees that the code can detect up to 2 errors and correct 1 error.</li>
  <li><strong>Rate of the Code:</strong> The rate of the code is defined as \( R = \frac{k}{n} \). For this design, the rate will be \( R = \frac{8}{12} = \frac{2}{3} \).</li>
</ul>

<h4>2. Generator Matrix (G)</h4>
<p>The generator matrix \( G \) is used to encode the message block into a codeword. For an \([n, k]\) linear block code, the generator matrix \( G \) is a \( k \times n \) matrix. We need to design \( G \) such that it generates codewords with a minimum distance of 3.</p>

<p>A possible generator matrix \( G \) for this code can be written as:</p>

<pre>
G = [1 0 0 0 1 1 0 1 0 0 1 0
     0 1 0 0 1 0 1 0 1 0 0 1
     0 0 1 0 1 0 0 1 0 1 0 1
     0 0 0 1 1 1 1 0 0 1 0 0
</pre>

<p>In this case, the matrix \( G \) is designed to ensure that the codewords generated from it have a minimum distance of 3.</p>

<h4>3. Parity-Check Matrix (H)</h4>
<p>The parity-check matrix \( H \) is related to the generator matrix \( G \) and is used to check the validity of codewords. For a linear block code, the parity-check matrix \( H \) is an \( (n - k) \times n \) matrix. The matrix \( H \) is designed to be orthogonal to \( G \), meaning that \( G X H^T = 0 \).</p>

<p>A corresponding parity-check matrix \( H \) for the code could be:</p>

<pre>
H = [1 0 1 0 0 1 1 0 0 1 0 0
     0 1 0 1 0 0 0 1 0 1 1 0
     0 0 1 0 1 0 1 0 1 0 1 0]
</pre>

<p>With this \( H \), the codewords generated by the generator matrix \( G \) will satisfy the condition \( C X H^T = 0 \), meaning that they are valid codewords.</p>

<h4>4. Codeword Example</h4>
<p>To verify the design, let’s generate an example codeword using the generator matrix \( G \). Assume the message block is \( D = [1, 0, 1, 1, 0, 1, 0, 1] \). The encoded codeword \( C \) is given by:</p>

<pre>
C = D · G = [1 0 1 1 0 1 0 1] · [1 0 0 0 1 1 0 1 0 0 1 0
                           0 1 0 0 1 0 1 0 1 0 0 1
                           0 0 1 0 1 0 0 1 0 1 0 1
                           0 0 0 1 1 1 1 0 0 1 0 0]
</pre>

<p>The resulting codeword will be a 12-bit vector.</p>

<h4>5. Error Detection and Correction</h4>
<p>With a minimum distance of 3, this code can detect up to 2 errors in a codeword and can correct 1 error. If 2 or fewer bits in a transmitted codeword are altered due to noise or errors, the receiver will be able to detect the errors and possibly correct one of them using the parity-check matrix \( H \). This is achieved because any two distinct codewords in the code differ by at least 3 bits, allowing the receiver to identify and correct errors.</p>

<h3>Summary</h3>
<p>The design of the linear block code is as follows:</p>
<ul>
  <li><strong>Message block size (k):</strong> 8 bits</li>
  <li><strong>Codeword length (n):</strong> 12 bits</li>
  <li><strong>Minimum distance (d):</strong> 3</li>
  <li><strong>Rate (R):</strong> 2/3</li>
  <li><strong>Generator matrix (G):</strong> as shown above</li>
  <li><strong>Parity-check matrix (H):</strong> as shown above</li>
</ul>

<p>This code is capable of detecting 2 errors and correcting 1 error, making it a reliable code for error correction in communication systems.</p>


---

### 3. If ‘C’ is a valid code vector such as **C = DG**, then prove that **C * H^T = 0**, where H is the parity check matrix.

<p><strong>Proof: If C is a valid code vector such that C = DG, then prove that C · H<sup>T</sup> = 0, where H is the parity check matrix.</strong></p>

<p>To prove that if \( C \) is a valid code vector such that \( C = DG \), then \( C X H^T = 0 \), where \( H \) is the parity check matrix, let's proceed step by step.</p>

<h3>Definitions:</h3>
<ul>
  <li><strong>Codeword:</strong> A valid codeword \( C \) is a vector in the code space. It can be written as \( C = DG \), where \( D \) is some message vector and \( G \) is the generator matrix.</li>
  <li><strong>Generator Matrix (G):</strong> The generator matrix \( G \) is used to encode the message vector \( D \) into a codeword \( C \).</li>
  <li><strong>Parity Check Matrix (H):</strong> The parity check matrix \( H \) is related to the generator matrix \( G \) of a linear block code. The codeword \( C \) satisfies the equation:
    <pre>
    C X H^T = 0
    </pre>
    This equation ensures that \( C \) is a valid codeword, meaning that it satisfies the parity check constraints.
  </li>
</ul>

<h3>Step-by-step Proof:</h3>

<ol>
  <li><strong>Start with the Codeword Expression:</strong>
    We are given that \( C = DG \), where \( D \) is the message vector and \( G \) is the generator matrix.
  </li>

  <li><strong>Apply the Parity Check Matrix:</strong>
    Now, we need to prove that \( C X H^T = 0 \). Substitute the expression for \( C \) into this equation:
    <pre>
    (DG) X H^T = 0
    </pre>
  </li>

  <li><strong>Use Associative Property:</strong>
    Matrix multiplication is associative, so we can rewrite the above expression as:
    <pre>
    D X (G X H^T) = 0
    </pre>
  </li>

  <li><strong>Relate the Generator and Parity Check Matrices:</strong>
    By the properties of the generator matrix \( G \) and the parity check matrix \( H \) for a linear code, we know that:
    <pre>
    G X H^T = 0
    </pre>
    This property is true because \( G \) and \( H \) are defined such that the codewords encoded using \( G \) are orthogonal to the rows of \( H \), which is the condition for a valid codeword.
  </li>

  <li><strong>Substitute the Zero Result:</strong>
    Since \( G X H^T = 0 \), the equation becomes:
    <pre>
    D X 0 = 0
    </pre>
    Which simplifies to:
    <pre>
    0 = 0
    </pre>
  </li>
</ol>

<h3>Conclusion:</h3>
<p>Since the equation \( C X H^T = 0 \) holds true for any codeword \( C = DG \), we have proven that if \( C \) is a valid codeword, then \( C X H^T = 0 \), where \( H \) is the parity check matrix. This shows that the codeword is orthogonal to the rows of the parity check matrix, as required for a valid code.</p>

---

### 4. In a linear block code, the syndrome is given by:
   - **S1 = r1 + r2 + r3 + r5**
   - **S2 = r1 + r2 + r4 + r6**
   - **S3 = r1 + r3 + r4 + r7**
   - Find:
     - Generator matrix [G]
     - Parity check matrix [H]
     - Write encoder and decoder circuit.
     - Find the code word for all the messages.
     - How many errors it can detect and correct.
     - Write the standard array.
     - Find the syndrome for the received data **1011 011**.

### 5. Consider a (3,1,2) Convolution Encoder with **g(1) = 110**, **g(2) = 101**, and **g(3) = 111**:
   - (i) Draw the encoder diagram.
   - (ii) Find the code word for the message sequence **(11101)** using the Generator Matrix and Transform domain approach.

### 6. For a (2,1,3) Convolution Encoder with **g(1) = 1101** and **g(2) = 1011**:
   - (i) Write the transition table.
   - (ii) State diagram.
   - (iii) Draw the code tree.
   - (iv) Draw the trellis diagram.
   - (v) Find the encoded output for the message **(11101)** by traversing the code tree.

### 7. For a (7,4) binary cyclic code, the generator polynomial is given by **g(x) = 1 + x + x³**:
   - Find the generator and parity check matrices.

### 8. A (15,5) linear cyclic code has a generator polynomial **g(x) = 1 + x + x² + x⁴ + x⁵ + x⁸ + x¹⁰**:
   - Draw the block diagram of an encoder and syndrome calculator circuit for this code.
   - Find the code polynomial for the message polynomial **D(x) = 1 + x² + x⁴** (in systematic form).
   - Is **V(x) = 1 + x⁴ + x⁶ + x⁸ + x¹⁴** a code polynomial? If not, find the syndrome of **V(x)**.

### 9. In a (7,4) binary cyclic code, the generator polynomial is given by **g(x) = 1 + x + x³**:
   - Find the codeword for messages **(1001)** and **(1011)**.
   - Show the contents of registers at each step.

### 10. Derive the expression for the probability of error considering coherent Binary Phase Shift Keying (BPSK) signal.

---

### 11. An FSK system transmits binary data at the rate of **2 x 10⁶ bps**. During transmission, AWGN with zero mean and two-sided PSD **10⁻²⁰ W/Hz** is added to the signal. The amplitude of the received sinusoidal wave for digit 1 or 0 is **1 μV**. Determine the average probability of symbol error assuming non-coherent detection.

<p>To determine the average probability of symbol error for an FSK system with non-coherent detection, we can use the following formula for the probability of symbol error:</p>

<pre>
P_e = Q(√(2E_b / N_0))
</pre>

<p>Where:</p>
<ul>
    <li><strong>P_e</strong> is the probability of symbol error.</li>
    <li><strong>Q(x)</strong> is the Q-function, which is related to the tail probability of the Gaussian distribution.</li>
    <li><strong>E_b</strong> is the energy per bit.</li>
    <li><strong>N_0</strong> is the two-sided power spectral density (PSD) of the noise.</li>
</ul>

<h3>Step 1: Calculate the energy per bit E_b</h3>

<p>The energy per bit E_b is related to the received signal amplitude A by the following formula:</p>

<pre>
E_b = A² / R_b
</pre>

<p>Where:</p>
<ul>
    <li><strong>A</strong> is the amplitude of the received signal.</li>
    <li><strong>R_b</strong> is the bit rate.</li>
</ul>

<p>Given:</p>
<ul>
    <li><strong>A = 1 μV = 1 × 10<sup>-6</sup> V.</li>
    <li><strong>R_b = 2 × 10<sup>6</sup> bps.</li>
</ul>

<p>Substitute the values into the equation:</p>

<pre>
E_b = (1 × 10<sup>-6</sup>)² / (2 × 10<sup>6</sup>) = (1 × 10<sup>-12</sup>) / (2 × 10<sup>6</sup>) = 5 × 10<sup>-19</sup> Joules.
</pre>

<h3>Step 2: Use the noise power spectral density N_0</h3>

<p>The two-sided power spectral density N<sub>0</sub> is given as:</p>

<pre>
N_0 = 10<sup>-20</sup> W/Hz.
</pre>

<h3>Step 3: Calculate the argument of the Q-function</h3>

<p>We now need to calculate the argument of the Q-function:</p>

<pre>
√(2E_b / N_0) = √((2 × 5 × 10<sup>-19</sup>) / 10<sup>-20</sup>) = √10 ≈ 3.162.
</pre>

<h3>Step 4: Calculate the probability of symbol error</h3>

<p>The probability of symbol error is:</p>

<pre>
P_e = Q(3.162).
</pre>

<p>Using a standard Q-function table or calculator:</p>

<pre>
Q(3.162) ≈ 0.00078.
</pre>

<h3>Final Answer:</h3>

<p>The average probability of symbol error is approximately:</p>

<pre>
P_e ≈ 0.00078.
</pre>

---

### 12. A binary data is transmitted over AWGN channel using BPSK at the rate of **2 Mbps**. It is desired to have average probability of error **Pe ≤ 10⁻⁵**, with noise spectral density **No/2 = 10⁻¹² W/Hz**. Determine the average carrier power required at the receiver input if the detector is of coherent type. Take **erfc(3.5) = 0.00025**.

<p>To determine the average carrier power required at the receiver input for a BPSK system under AWGN conditions, we can use the following formula for the probability of error in a BPSK system with coherent detection:</p>

<pre>
P_e = (1/2) * erfc(√(E_b / N_0))
</pre>

<p>Where:</p>
<ul>
    <li><strong>P_e</strong> is the probability of error.</li>
    <li><strong>erfc(x)</strong> is the complementary error function.</li>
    <li><strong>E_b</strong> is the energy per bit.</li>
    <li><strong>N_0</strong> is the noise power spectral density.</li>
</ul>

<h3>Step 1: Given values</h3>

<ul>
    <li><strong>P_e = 10<sup>-5</sup></strong></li>
    <li><strong>N<sub>0</sub>/2 = 10<sup>-12</sup> W/Hz, so N<sub>0</sub> = 2 × 10<sup>-12</sup> W/Hz.</li>
    <li><strong>Bit rate R<sub>b</sub> = 2 Mbps = 2 × 10<sup>6</sup> bps.</li>
    <li><strong>erfc(3.5) = 0.00025</strong></li>
</ul>

<h3>Step 2: Relating P<sub>e</sub> to erfc</h3>

<p>From the equation for the probability of error:</p>

<pre>
P_e = (1/2) * erfc(√(E_b / N_0))
</pre>

<p>Rearranging for erfc(√(E_b / N_0)):</p>

<pre>
erfc(√(E_b / N_0)) = 2 * P_e
</pre>

<p>Substitute the given P<sub>e</sub> = 10<sup>-5</sup>:</p>

<pre>
erfc(√(E_b / N_0)) = 2 * 10<sup>-5</sup>
</pre>

<h3>Step 3: Solve for √(E<sub>b</sub> / N<sub>0</sub>)</h3>

<p>We are given that erfc(3.5) = 0.00025, so we approximate:</p>

<pre>
erfc(x) ≈ 2 * 10<sup>-5</sup> ⇒ x ≈ 3.5
</pre>

<p>Thus:</p>

<pre>
√(E_b / N_0) = 3.5
</pre>

<h3>Step 4: Solve for E<sub>b</sub></h3>

<p>Now square both sides of the equation:</p>

<pre>
(E_b / N_0) = 3.5<sup>2</sup> = 12.25
</pre>

<p>So:</p>

<pre>
E_b = 12.25 * N_0
</pre>

<p>Substitute N<sub>0</sub> = 2 × 10<sup>-12</sup> W/Hz:</p>

<pre>
E_b = 12.25 * 2 × 10<sup>-12</sup> = 24.5 × 10<sup>-12</sup> Joules
</pre>

<h3>Step 5: Calculate the average carrier power</h3>

<p>The energy per bit E<sub>b</sub> is related to the carrier power P<sub>c</sub> by the equation:</p>

<pre>
E_b = P_c / R_b
</pre>

<p>Where:</p>
<ul>
    <li><strong>P_c</strong> is the average carrier power.</li>
    <li><strong>R_b</strong> is the bit rate.</li>
</ul>

<p>Rearrange the equation to solve for P<sub>c</sub>:</p>

<pre>
P_c = E_b * R_b
</pre>

<p>Substitute E<sub>b</sub> = 24.5 × 10<sup>-12</sup> Joules and R<sub>b</sub> = 2 × 10<sup>6</sup> bps:</p>

<pre>
P_c = 24.5 × 10<sup>-12</sup> * 2 × 10<sup>6</sup> = 49 × 10<sup>-6</sup> = 49 μW
</pre>

<h3>Final Answer:</h3>

<p>The average carrier power required at the receiver input is:</p>

<pre>
P_c = 49 μW.
</pre>

---

### 13. Explain Non-coherent BFSK detection with relevant equations and explanation.

<p><strong>Non-Coherent BFSK Detection</strong></p>

<p>Binary Frequency Shift Keying (BFSK) is a form of frequency modulation where two distinct frequencies represent two binary states (0 and 1). Non-coherent detection of BFSK refers to detecting the transmitted binary data without knowledge of the phase of the carrier signal. This means that the receiver does not have access to the phase of the received signal, unlike coherent detection, where phase synchronization is assumed.</p>

<h3>Principle of Non-Coherent BFSK Detection</h3>

<p>In non-coherent BFSK detection, the receiver uses the frequency differences between the two signals to determine which bit was transmitted. Since phase synchronization is not required, the receiver relies on comparing the energy received over each frequency during the symbol period.</p>

<h3>Key Concepts</h3>

<ul>
    <li><strong>Two Frequencies:</strong> 
        <ul>
            <li>The binary 1 is represented by a frequency <strong>f<sub>1</sub></strong>.</li>
            <li>The binary 0 is represented by a frequency <strong>f<sub>0</sub></strong>.</li>
        </ul>
    </li>
    <li><strong>Non-Coherent Detection:</strong> The receiver doesn’t know the phase of the signal at any point, so it must make decisions based solely on the signal's energy in each frequency band.</li>
</ul>

<h3>Signal Model</h3>

<p>The transmitted signal for BFSK can be expressed as:</p>

<ul>
    <li>For bit 1 (represented by frequency <strong>f<sub>1</sub></strong>):</li>
    <pre>
    s<sub>1</sub>(t) = √(2E<sub>b</sub>) · cos(2π f<sub>1</sub> t + φ)
    </pre>
    
    <li>For bit 0 (represented by frequency <strong>f<sub>0</sub></strong>):</li>
    <pre>
    s<sub>0</sub>(t) = √(2E<sub>b</sub>) · cos(2π f<sub>0</sub> t + φ)
    </pre>
</ul>

<p>Where:</p>
<ul>
    <li><strong>E<sub>b</sub></strong> is the energy per bit.</li>
    <li><strong>f<sub>0</sub></strong> and <strong>f<sub>1</sub></strong> are the frequencies corresponding to bit 0 and bit 1, respectively.</li>
    <li><strong>φ</strong> is the phase, which is unknown in non-coherent detection.</li>
</ul>

<p>The signal is corrupted by noise, typically modeled as Additive White Gaussian Noise (AWGN).</p>

<h3>Detection Process</h3>

<p>The receiver needs to determine whether bit 1 or bit 0 was transmitted based on the received signal. For non-coherent detection, this is done by comparing the <strong>energy</strong> received during a symbol period for both frequencies <strong>f<sub>0</sub></strong> and <strong>f<sub>1</sub></strong>.</p>

<h4>1. Matched Filter Detection</h4>
<p>A matched filter is used for each frequency, and the received signal is correlated with the expected waveform for that frequency. The energy for each frequency is computed over the symbol period <strong>T</strong>.</p>

<h4>2. Energy Computation</h4>
<p>The decision is based on the energies <strong>E<sub>0</sub></strong> and <strong>E<sub>1</sub></strong> computed for the frequencies <strong>f<sub>0</sub></strong> and <strong>f<sub>1</sub></strong>, respectively. The energy for a given frequency is:</p>
<pre>
E<sub>f</sub> = ∫<sub>0</sub><sup>T</sup> |r(t) · cos(2π f t)|² dt
</pre>

<p>Where:</p>
<ul>
    <li><strong>r(t)</strong> is the received signal.</li>
    <li><strong>f</strong> is either <strong>f<sub>0</sub></strong> or <strong>f<sub>1</sub></strong>, depending on the hypothesis.</li>
</ul>

<h4>3. Decision Rule</h4>
<p>The receiver computes the energies <strong>E<sub>0</sub></strong> and <strong>E<sub>1</sub></strong> and chooses the bit corresponding to the frequency that has the highest energy. Specifically:</p>
<ul>
    <li>If <strong>E<sub>1</sub> > E<sub>0</sub>, bit 1 is detected.</li>
    <li>If <strong>E<sub>0</sub> > E<sub>1</sub>, bit 0 is detected.</li>
</ul>

<h3>Probability of Error</h3>

<p>The probability of error for non-coherent BFSK detection is influenced by the signal-to-noise ratio (SNR), which is a function of the energy per bit <strong>E<sub>b</sub></strong> and the noise power spectral density <strong>N<sub>0</sub></strong>.</p>

<p>The probability of error <strong>P<sub>e</sub></strong> for non-coherent detection is given by:</p>
<pre>
P<sub>e</sub> = Q(√(E<sub>b</sub> / N<sub>0</sub>))
</pre>

Where:
<ul>
    <li><strong>Q(x)</strong> is the Q-function, which represents the tail probability of the Gaussian distribution.</li>
</ul>

<h3>Energy Detection Method</h3>

<p>For non-coherent detection, the energy of the signal is the key parameter. The receiver essentially compares the total energy in the received signal during the symbol period for the two frequencies. The frequency corresponding to the greater energy will determine the transmitted bit.</p>

<h3>Key Points of Non-Coherent BFSK Detection</h3>

<ul>
    <li><strong>No Phase Knowledge:</strong> Non-coherent detection does not require knowledge of the phase of the received signal, which simplifies the detection process compared to coherent detection.</li>
    <li><strong>Energy Comparison:</strong> The decision is made based on comparing the energy of the signal received at two different frequencies.</li>
    <li><strong>Matched Filters:</strong> Matched filters are used for optimal detection, as they maximize the signal-to-noise ratio (SNR) at the receiver for each frequency.</li>
    <li><strong>Error Probability:</strong> The error probability depends on the ratio of the energy per bit <strong>E<sub>b</sub></strong> to the noise power spectral density <strong>N<sub>0</sub></strong>, and it can be calculated using the Q-function.</li>
</ul>

<h3>Summary</h3>

<p>Non-coherent BFSK detection is based on comparing the energy of the received signal at two distinct frequencies corresponding to the binary bits. It does not require phase synchronization, making it simpler than coherent detection. The probability of error in this system depends on the energy per bit and the noise spectral density, and the error is minimized by using matched filters for each frequency.</p>

---

### 14. Define the Hilbert Transform. State the properties of it.

### 15. Define the complex envelope of bandpass signals. Obtain the canonical representation of bandpass signals.

---

### 16. Explain the Gram-Schmidt Orthogonalization procedure.

<p><strong>Gram-Schmidt Orthogonalization Procedure</strong></p>

<p>The Gram-Schmidt Orthogonalization procedure is a method used in linear algebra to orthogonalize a set of vectors in an inner product space, which means transforming a set of linearly independent vectors into a set of orthogonal (perpendicular) vectors. This process is widely used in various fields such as signal processing, numerical analysis, and machine learning.</p>

<h3>Goal</h3>

<p>The goal of the Gram-Schmidt procedure is to take a set of linearly independent vectors and produce an orthogonal (or orthonormal) set of vectors that span the same subspace. These orthogonal vectors can then be used for more efficient computations or simplifications, such as solving systems of linear equations, eigenvalue problems, and more.</p>

<h3>Procedure</h3>

<p>Given a set of linearly independent vectors, <strong>{v₁, v₂, ..., vₖ}</strong>, the Gram-Schmidt procedure constructs an orthogonal set of vectors <strong>{u₁, u₂, ..., uₖ}</strong> by performing the following steps:</p>

<h4>Step 1: Initialize the First Vector</h4>

<p>Start by defining the first vector of the orthogonal set:</p>
<pre>
u₁ = v₁
</pre>

<h4>Step 2: Orthogonalize Subsequent Vectors</h4>

<p>For each vector <strong>vₖ</strong> in the original set (for <strong>k = 2, 3, ..., n</strong>), subtract the projection of <strong>vₖ</strong> onto each of the previous vectors <strong>u₁, u₂, ..., uₖ₋₁</strong> to ensure orthogonality. The projection of <strong>vₖ</strong> onto <strong>uᵢ</strong> is given by:</p>
<pre>
proj(uᵢ) = (vᵢ · uᵢ) / (uᵢ · uᵢ) * uᵢ
</pre>

<p>So, the new vector <strong>uₖ</strong> is:</p>
<pre>
uₖ = vₖ - ∑<sub>i=1</sub><sup>k-1</sup> proj(uᵢ)
</pre>

<h4>Step 3: Normalize (Optional)</h4>

<p>If you want to create an orthonormal set, normalize each <strong>uᵢ</strong> by dividing it by its magnitude:</p>
<pre>
eᵢ = uᵢ / ||uᵢ||
</pre>

<p>Now, the set <strong>{e₁, e₂, ..., eₖ}</strong> forms an orthonormal basis, where each vector has a unit length (norm = 1).</p>

<h3>Example</h3>

<p>Let’s illustrate the Gram-Schmidt procedure with a simple example. Consider the two linearly independent vectors <strong>v₁ = [1, 1]</strong> and <strong>v₂ = [1, 0]</strong>.</p>

<pre>
Step 1: Initialize the first vector:
u₁ = v₁ = [1, 1]

Step 2: Orthogonalize v₂:
proj(u₁) = (v₂ · u₁) / (u₁ · u₁) * u₁ = (1 * 1 + 0 * 1) / (1 * 1 + 1 * 1) * [1, 1] = 1/2 * [1, 1] = [1/2, 1/2]
u₂ = v₂ - proj(u₁) = [1, 0] - [1/2, 1/2] = [1/2, -1/2]

Step 3: Normalize (optional):
||u₁|| = √(1² + 1²) = √2
e₁ = u₁ / ||u₁|| = [1/√2, 1/√2]

||u₂|| = √((1/2)² + (-1/2)²) = 1/√2
e₂ = u₂ / ||u₂|| = [1/2, -1/2] / 1/√2 = [√2/2, -√2/2]
</pre>

<p>Now, the set of orthonormal vectors <strong>{e₁, e₂}</strong> is:</p>
<pre>
e₁ = [√2/2, √2/2]
e₂ = [√2/2, -√2/2]
</pre>

<h3>Key Points</h3>

<ul>
    <li><strong>Orthogonal Vectors:</strong> The vectors produced by the Gram-Schmidt process are orthogonal, meaning their inner product (dot product) is zero: <strong>uᵢ · uⱼ = 0</strong> for <strong>i ≠ j</strong>.</li>
    <li><strong>Orthonormal Vectors:</strong> If the vectors are also normalized, then they are orthonormal, meaning their norms are 1: <strong>||uᵢ|| = 1</strong>.</li>
    <li><strong>Applications:</strong> The Gram-Schmidt procedure is used in QR decomposition, solving linear systems, and in algorithms such as the Gram-Schmidt algorithm in numerical computing.</li>
</ul>

<h3>Summary</h3>

<p>The Gram-Schmidt Orthogonalization procedure is a fundamental algorithm used to generate an orthogonal (or orthonormal) set of vectors from a given set of linearly independent vectors. The process involves subtracting projections of the vectors onto each other and, if desired, normalizing them to produce orthonormal vectors.</p>

---

### 17. Derive the expressions for mean and variance of the correlator outputs. Also, show that the correlator outputs are statistically independent (or) the conversion of the Continuous AWGN Channel into a Vector Channel.

### 18. Derive the expression for the probability of error in a coherent **binary phase shift keying (BPSK)** system.
