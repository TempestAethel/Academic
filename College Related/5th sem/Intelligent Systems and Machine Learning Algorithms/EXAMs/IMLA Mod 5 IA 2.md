# Intelligent Systems and Machine Learning Algorithms (Module 5) - Question Bank

## Table of Contents

1. [Explain the below concepts with its code:](#q1)
   - a) Look at the big picture
   - b) Get the data
2. [Explain the below concepts with its code:](#q2)
   - a) Discover and visualize the data
   - b) Prepare the data
3. [Describe the steps involved in selection and training the model with an example.](#q3)
4. [Given a dataset of handwritten digits, outline the steps to preprocess the data, train a binary classifier to distinguish between the digits '0' and '1', and evaluate its performance.](#q4)
5. [Given a dataset of handwritten characters, outline the steps to preprocess the data, train a binary classifier to distinguish between small letter and capital letters, and evaluate its performance.](#q5)
6. [Describe the steps involved in preparing data for a machine learning model and construct a simple model with an example.](#q6)
7. [Build the machine learning model to fine-tune it with its code.](#q7)
8. [Make use of MNIST classification and explain the machine learning model.](#q8)
9. [Write a brief note on performance measure and error analysis with its appropriate equation.](#q9)
10. [Examine a model of fine-tuning technique in Python using Random search over 5-fold cross-validation for Random Forest algorithm assuming both training data with target values are provided.](#q10)
11. [Using code snippets, outline the concepts involved in:](#q11)
   - i) Measuring accuracy using Cross-Validation
   - ii) Confusion Matrix
   - iii) Precision and Recall
12. [Describe multiclass classifiers and explain strategies to implement multiclass classifiers from binary classifiers.](#q12)
13. [With the code snippet explain how Multilabel classification is different from multiclass and Multioutput classification?](#q13)
14. [List a few of the typical performance measures for regression problems.](#q14)

---

## Questions and Answers

<a id="q1"></a>
### 1. Explain the below concepts with its code:
   - a) **Look at the big picture** (8M)
     - Before diving into machine learning, it's crucial to first understand the problem you're trying to solve. Looking at the "big picture" involves understanding the data, the goal of the project, the type of machine learning technique you will need, and how your model will be used in the real world. This understanding can guide your decisions throughout the machine learning pipeline.
     - **Example**: Imagine you're working on a project to classify images of animals. The big picture involves understanding that this is a **classification** problem (because you're predicting categories) and may involve techniques like **deep learning** or **convolutional neural networks** for image data.
     - **Code Example** (Visualization of the problem):
       ```python
       import matplotlib.pyplot as plt
       import seaborn as sns
       sns.set(style="darkgrid")
       data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
       plt.plot(data)
       plt.title('Big Picture - Data Overview')
       plt.show()
       ```

   - b) **Get the data** (8M)
     - The next step is to gather the data that will be used for training and testing your model. Data can be sourced from CSV files, databases, or APIs. Understanding the structure and type of the data is essential for applying the appropriate preprocessing techniques.
     - **Example**: Suppose you're working with a dataset of customer information for a classification problem. The dataset might contain columns like `age`, `income`, `purchased`, etc. You'll need to understand this data and how it can be utilized for the model.
     - **Code Example** (Loading a dataset using pandas):
       ```python
       import pandas as pd
       data = pd.read_csv('dataset.csv')
       print(data.head())  # Show first 5 rows of data
       ```

---

<a id="q2"></a>
### 2. Explain the below concepts with its code:
   - a) **Discover and visualize the data** (8M)
     - Data exploration and visualization help you understand the dataset better. It helps identify patterns, correlations, missing values, and outliers in the data.
     - **Example**: Visualizing the distribution of a numerical column, like `age`, to understand the spread of data.
     - **Code Example** (Visualizing data with seaborn):
       ```python
       import seaborn as sns
       sns.histplot(data['age'])
       plt.title('Age Distribution')
       plt.show()
       ```

   - b) **Prepare the data** (8M)
     - Data preparation includes steps like handling missing values, scaling features, and encoding categorical data. It’s essential to clean the data and transform it into a format suitable for the model.
     - **Example**: Handling missing values and scaling numerical features.
     - **Code Example** (Imputation and scaling):
       ```python
       from sklearn.preprocessing import StandardScaler
       data = data.fillna(data.mean())  # Impute missing values with column mean
       scaler = StandardScaler()
       data_scaled = scaler.fit_transform(data)
       ```

---

<a id="q3"></a>
### 3. Describe the steps involved in selection and training the model with an example. (8M)

The steps involved in model selection and training are as follows:

**Step 1: Data Preprocessing**
   - This step involves cleaning and preparing the data, such as handling missing values, normalizing or scaling features, encoding categorical data, etc.

**Step 2: Model Selection**
   - Choose a machine learning model based on the nature of the problem. For classification problems, you could use models like Logistic Regression, Decision Trees, Random Forest, or Support Vector Machines (SVM). For regression problems, models like Linear Regression, Decision Trees, or Random Forest may be appropriate.

**Step 3: Data Splitting**
   - Split the dataset into training and testing sets, typically using an 80-20 split or a 70-30 split. This ensures that the model is trained on one subset of data and evaluated on another unseen subset.

**Step 4: Model Training**
   - Train the selected model using the training dataset. This involves feeding the data into the model and allowing it to learn the relationships between the input features and the target variable.

**Step 5: Model Evaluation**
   - After training, evaluate the model on the test data to assess its performance. This can be done using accuracy, precision, recall, F1-score for classification, or Mean Absolute Error (MAE), Mean Squared Error (MSE) for regression tasks.

**Step 6: Model Tuning**
   - Optionally, fine-tune the model using hyperparameter optimization techniques like Grid Search or Random Search to achieve better performance.

**Example Code** (Random Forest for classification):
```
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Load the dataset (assuming 'data' is a Pandas DataFrame)
X = data.drop('target', axis=1)
y = data['target']

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Initialize the model
model = RandomForestClassifier()

# Train the model
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)

# Evaluate accuracy
print(f"Accuracy: {accuracy_score(y_test, predictions)}")
```

---

<a id="q4"></a>
### 4. Given a dataset of handwritten digits, outline the steps to preprocess the data, train a binary classifier to distinguish between the digits '0' and '1', and evaluate its performance. (10M)

**Step 1: Data Preprocessing**
   - **Load the Dataset**: First, we load the handwritten digits dataset, such as MNIST, which contains images of handwritten digits.
   - **Data Cleaning**: Check for any missing or corrupted data. Since MNIST is a clean dataset, this step is generally not needed.
   - **Feature Scaling**: Normalize or scale the image data, as pixel values range from 0 to 255, which might not work well with some models.

**Step 2: Data Transformation**
   - **Binary Classification**: Convert the target labels to distinguish between the digits '0' and '1'. You can create a binary classification task by mapping all digits other than '0' or '1' as a negative class.
   - **Flatten Images**: The images are 28x28 pixels. You need to flatten them into a one-dimensional array for each image.

**Step 3: Split the Data**
   - Divide the data into training and testing sets.

**Step 4: Model Selection**
   - For binary classification, you can use algorithms such as Logistic Regression, Random Forest, or Support Vector Machine (SVM).

**Step 5: Train the Model**
   - Train the chosen binary classifier using the training dataset.

**Step 6: Evaluate the Model**
   - Evaluate the performance using metrics like accuracy, precision, recall, and F1-score.

**Example Code** (Using Logistic Regression for binary classification):
```
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import fetch_openml

# Load MNIST dataset
mnist = fetch_openml('mnist_784')

# Preprocess: Select only digits 0 and 1
X = mnist.data[(mnist.target == '0') | (mnist.target == '1')]
y = mnist.target[(mnist.target == '0') | (mnist.target == '1')]

# Flatten the images
X = X / 255.0  # Normalize pixel values to between 0 and 1

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model: Logistic Regression
model = LogisticRegression(max_iter=10000)

# Train the model
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)

# Evaluate accuracy
print(f"Accuracy: {accuracy_score(y_test, predictions)}")
print("Classification Report:")
print(classification_report(y_test, predictions))
```

---

<a id="q5"></a>
### 5. Given a dataset of handwritten characters, outline the steps to preprocess the data, train a binary classifier to distinguish between small letters and capital letters, and evaluate its performance. (10M)

**Step 1: Data Preprocessing**
   - **Load the Dataset**: Load the dataset containing handwritten characters, which may be in the form of images or a CSV file containing pixel values.
   - **Handle Missing Data**: Check for any missing data or corrupted images.
   - **Feature Scaling**: Normalize the pixel values to a common range (e.g., [0, 1]).

**Step 2: Data Transformation**
   - **Label Transformation**: For binary classification, label lowercase letters (e.g., 'a', 'b', 'c', ...) as one class and uppercase letters (e.g., 'A', 'B', 'C', ...) as the other class.
   - **Flattening Images**: If the dataset contains images, flatten each image to a one-dimensional vector.

**Step 3: Data Splitting**
   - Split the data into training and testing sets, ensuring that both classes (lowercase and uppercase) are well-represented in each set.

**Step 4: Model Selection**
   - Choose an appropriate model like Logistic Regression, SVM, or Random Forest for binary classification.

**Step 5: Model Training**
   - Train the classifier using the training set.

**Step 6: Model Evaluation**
   - Evaluate the model using appropriate metrics such as accuracy, precision, recall, and F1-score.

**Example Code** (Using Random Forest for binary classification):
```
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.datasets import fetch_openml

# Load the dataset (assuming the data is structured similarly to MNIST)
dataset = fetch_openml('letters')  # Example dataset of handwritten letters

# Preprocess: Label lowercase vs uppercase letters
X = dataset.data
y = dataset.target
y = (y.str.islower()).astype(int)  # Convert labels to 0 for lowercase, 1 for uppercase

# Normalize the pixel values
X = X / 255.0  # Normalize pixel values between 0 and 1

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model: Random Forest
model = RandomForestClassifier()

# Train the model
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)

# Evaluate accuracy
print(f"Accuracy: {accuracy_score(y_test, predictions)}")
print("Classification Report:")
print(classification_report(y_test, predictions))
```

---

<a id="q6"></a>
### 6. Describe the steps involved in preparing data for a machine learning model and construct a simple model with an example. (10M)

**Step 1: Data Collection**
   - Gather the dataset that will be used to train the model. This could be from a publicly available dataset or proprietary data.

**Step 2: Data Preprocessing**
   - **Handling Missing Data**: Check if there are any missing values in the dataset. You can fill in missing values with a mean, median, or mode, or drop rows or columns with missing values.
   - **Feature Scaling**: Standardize or normalize the features so they all have similar scales. This is particularly important for algorithms like k-NN or SVM.
   - **Encoding Categorical Data**: Convert categorical variables to numerical format using techniques like one-hot encoding or label encoding.
   - **Feature Engineering**: Create new features or transform existing ones to improve the model’s performance.

**Step 3: Data Splitting**
   - Split the data into training and test sets, typically using an 80-20 split.

**Step 4: Model Selection**
   - Choose a machine learning model. For this example, we'll use a Logistic Regression model for classification.

**Step 5: Train the Model**
   - Train the model using the training data.

**Step 6: Model Evaluation**
   - Evaluate the model using appropriate metrics like accuracy, precision, recall, or F1-score.

**Example Code** (Logistic Regression for classification):
```
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris

# Load the Iris dataset
data = load_iris()
X = data.data
y = data.target

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the model
model = LogisticRegression(max_iter=10000)

# Train the model
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)

# Evaluate accuracy
print(f"Accuracy: {accuracy_score(y_test, predictions)}")
```

---

<a id="q7"></a>
### 7. Build the machine learning model to fine-tune it with its code. (10M)

Fine-tuning a machine learning model typically involves optimizing hyperparameters to improve performance. Here's how to fine-tune a model like Random Forest using Grid Search for hyperparameter optimization.

**Step 1: Load and Preprocess Data**
   - Load and preprocess the data as shown in the previous example, ensuring that missing values are handled, features are scaled, and categorical variables are encoded.

**Step 2: Split Data**
   - Split the dataset into training and testing sets.

**Step 3: Model Selection**
   - Choose a model to fine-tune. For this example, we use a Random Forest Classifier.

**Step 4: Hyperparameter Tuning Using Grid Search**
   - Use `GridSearchCV` to search through a grid of hyperparameters to find the optimal values.

**Step 5: Train the Model**
   - Train the model with the best hyperparameters found during grid search.

**Step 6: Evaluate the Model**
   - Evaluate the model using appropriate metrics like accuracy or F1-score.

**Example Code** (Random Forest with Grid Search):
```
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris

# Load dataset
data = load_iris()
X = data.data
y = data.target

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the RandomForest model
rf = RandomForestClassifier()

# Hyperparameter grid
param_grid = {
    'n_estimators': [10, 50, 100],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# GridSearchCV for hyperparameter tuning
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, verbose=2, n_jobs=-1)

# Train the model with GridSearch
grid_search.fit(X_train, y_train)

# Best parameters
print("Best Parameters:", grid_search.best_params_)

# Predict and evaluate
best_model = grid_search.best_estimator_
predictions = best_model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, predictions)}")
```

---

<a id="q8"></a>
### 8. Make use of MNIST classification and explain the machine learning model. (7M)

The MNIST dataset consists of handwritten digits (0-9) and is commonly used to evaluate machine learning models in image classification.

**Step 1: Load the MNIST Dataset**
   - Load the MNIST dataset using libraries like `sklearn` or `tensorflow`.

**Step 2: Data Preprocessing**
   - **Normalization**: Scale the pixel values of the images to a range between 0 and 1.
   - **Flattening**: Each image in the MNIST dataset is 28x28 pixels. For machine learning algorithms, this 2D image is usually flattened into a 1D array of 784 pixels.

**Step 3: Split Data**
   - Split the dataset into training and testing sets.

**Step 4: Model Selection**
   - For MNIST, a common model used is a neural network (e.g., Multi-layer Perceptron or Convolutional Neural Network).

**Step 5: Train the Model**
   - Train the model using the training set.

**Step 6: Model Evaluation**
   - Evaluate the model on the test data using accuracy and confusion matrix.

**Example Code** (Using a simple neural network model with `sklearn`):
```
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from sklearn.datasets import fetch_openml

# Load the MNIST dataset
mnist = fetch_openml('mnist_784')

# Preprocess: Normalize the images
X = mnist.data / 255.0
y = mnist.target

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Multi-layer Perceptron Classifier
model = MLPClassifier(hidden_layer_sizes=(64, 64), max_iter=20)

# Train the model
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)

# Evaluate accuracy
print(f"Accuracy: {accuracy_score(y_test, predictions)}")
```

---

<a id="q9"></a>
### 9. Write a brief note on performance measure and error analysis with its appropriate equation. (10M)

**Performance Measures**:

Performance measures evaluate how well a machine learning model is performing. These measures depend on the problem type (classification or regression).

**For Classification Problems**:
1. **Accuracy**: The proportion of correct predictions to the total number of predictions.
   \[
   \text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Predictions}}
   \]
2. **Precision**: The proportion of positive predictions that are actually correct.
   \[
   \text{Precision} = \frac{TP}{TP + FP}
   \]
3. **Recall (Sensitivity or True Positive Rate)**: The proportion of actual positives that are correctly identified.
   \[
   \text{Recall} = \frac{TP}{TP + FN}
   \]
4. **F1-Score**: The harmonic mean of precision and recall.
   \[
   \text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
   \]
5. **Confusion Matrix**: A table that summarizes the classification performance by showing true positives, false positives, true negatives, and false negatives.

**For Regression Problems**:
1. **Mean Absolute Error (MAE)**: The average of absolute errors between predicted and actual values.
   \[
   \text{MAE} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|
   \]
2. **Mean Squared Error (MSE)**: The average of the squared differences between predicted and actual values.
   \[
   \text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
   \]
3. **R-squared (R²)**: A measure of how well the model explains the variance in the data.
   \[
   R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (y_i - \bar{y})^2}
   \]

**Error Analysis**:
   - Error analysis helps understand where the model is making mistakes and why. It involves reviewing misclassified examples (for classification) or large residuals (for regression) to understand patterns.
   - Techniques like residual plots, error histograms, and confusion matrices help visualize the sources of errors.

---

<a id="q10"></a>
### 10. Examine a model of fine-tuning technique in Python using Random search over 5-fold cross-validation for Random Forest algorithm assuming both training data with target values are provided. (10M)

**Step 1: Import Libraries**
   - Import necessary libraries like `RandomForestClassifier`, `RandomizedSearchCV`, and `train_test_split`.

**Step 2: Load the Data**
   - Load the dataset (training data and target values).

**Step 3: Split the Data**
   - Split the data into training and testing sets.

**Step 4: Hyperparameter Tuning with RandomizedSearchCV**
   - Use `RandomizedSearchCV` to randomly search the hyperparameter space for the Random Forest model. This will perform random sampling of hyperparameters, which is computationally efficient compared to grid search.

**Step 5: Cross-Validation**
   - Use 5-fold cross-validation to train the model and evaluate performance.

**Step 6: Train the Model**
   - Train the Random Forest model using the best hyperparameters found from Random Search.

**Example Code** (Random Forest with RandomizedSearchCV and 5-fold Cross-Validation):
```
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris
from scipy.stats import randint

# Load the Iris dataset
data = load_iris()
X = data.data
y = data.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest Classifier
rf = RandomForestClassifier()

# Hyperparameter grid for RandomizedSearchCV
param_dist = {
    'n_estimators': randint(10, 200),
    'max_depth': randint(1, 20),
    'min_samples_split': randint(2, 10),
    'min_samples_leaf': randint(1, 10)
}

# Initialize RandomizedSearchCV with 5-fold cross-validation
random_search = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=50, cv=5, random_state=42, n_jobs=-1)

# Fit the model with RandomizedSearchCV
random_search.fit(X_train, y_train)

# Best parameters found from RandomizedSearchCV
print("Best Parameters:", random_search.best_params_)

# Predict using the best model
best_model = random_search.best_estimator_
predictions = best_model.predict(X_test)

# Evaluate accuracy
print(f"Accuracy: {accuracy_score(y_test, predictions)}")
```

---

<a id="q11"></a>
### 11. Using code snippets, outline the concepts involved in 
#### i) Measuring accuracy using Cross-Validation, 
#### ii) Confusion Matrix, and
#### iii) Precision and Recall (6M)

**i) Measuring Accuracy Using Cross-Validation**

Cross-validation is used to evaluate the performance of a machine learning model by splitting the dataset into multiple folds (usually 5 or 10) and training and testing the model on each fold.

**Example Code** (Using KFold cross-validation for accuracy measurement):
```
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris

# Load dataset
data = load_iris()
X = data.data
y = data.target

# Initialize the model
model = LogisticRegression(max_iter=10000)

# Measure accuracy using 5-fold cross-validation
cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')

# Print the cross-validation results
print(f"Accuracy for each fold: {cv_scores}")
print(f"Average accuracy: {cv_scores.mean()}")
```

**ii) Confusion Matrix**

A confusion matrix is used to evaluate the performance of classification models by displaying the actual versus predicted classifications in a matrix form.

**Example Code** (Using confusion matrix):
```
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris

# Load dataset
data = load_iris()
X = data.data
y = data.target

# Split data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Train the model
model = LogisticRegression(max_iter=10000)
model.fit(X_train, y_train)

# Predict the labels
predictions = model.predict(X_test)

# Confusion matrix
cm = confusion_matrix(y_test, predictions)
print("Confusion Matrix:")
print(cm)
```

**iii) Precision and Recall**

Precision and recall are performance metrics used for evaluating classification models, especially in imbalanced datasets.

**Example Code** (Using precision and recall):
```
from sklearn.metrics import precision_score, recall_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris

# Load dataset
data = load_iris()
X = data.data
y = data.target

# Split data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Train the model
model = LogisticRegression(max_iter=10000)
model.fit(X_train, y_train)

# Predict the labels
predictions = model.predict(X_test)

# Calculate Precision and Recall
precision = precision_score(y_test, predictions, average='weighted')
recall = recall_score(y_test, predictions, average='weighted')

print(f"Precision: {precision}")
print(f"Recall: {recall}")
```

---

<a id="q12"></a>
### 12. Describe multiclass classifiers and explain strategies to implement multiclass classifiers from binary classifiers. (8M)

**Multiclass Classifiers**:
A multiclass classifier is designed to handle problems where there are more than two classes. Unlike binary classification, which deals with two classes (e.g., positive vs negative), multiclass classification involves more than two possible categories or outcomes.

**Strategies to Implement Multiclass Classifiers from Binary Classifiers**:
1. **One-vs-Rest (OvR) / One-vs-All (OvA)**:
   - In this approach, a separate binary classifier is trained for each class. Each classifier distinguishes one class from all others.
   - Example: For a 3-class classification problem, three binary classifiers are trained, each classifying whether an example belongs to a specific class or not.
   
   **How it works**:
   - Train one binary classifier for class 0 vs (class 1 + class 2).
   - Train another binary classifier for class 1 vs (class 0 + class 2).
   - Train a third binary classifier for class 2 vs (class 0 + class 1).
   
   During prediction, the classifier with the highest confidence in its prediction determines the predicted class.

2. **One-vs-One (OvO)**:
   - This approach trains a separate classifier for each pair of classes. For a dataset with `n` classes, it involves training `n(n-1)/2` classifiers.
   - Example: For a 3-class problem, you'd train 3 classifiers: class 0 vs class 1, class 0 vs class 2, and class 1 vs class 2.
   
   **How it works**:
   - For each prediction, the class with the most "votes" (i.e., the majority of classifiers agree on the class) is chosen.

3. **Multinomial Classification**:
   - Some algorithms, like multinomial logistic regression and decision trees, can naturally handle multiple classes by extending binary classification techniques.
   - Example: Logistic Regression with softmax activation allows you to handle multiple classes without converting the problem into binary classifiers.

**Example Code** (One-vs-Rest with Logistic Regression):
```
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import accuracy_score

# Load Iris dataset
data = load_iris()
X = data.data
y = data.target

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize One-vs-Rest Logistic Regression model
model = OneVsRestClassifier(LogisticRegression(max_iter=10000))

# Train the model
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)

# Evaluate accuracy
print(f"Accuracy: {accuracy_score(y_test, predictions)}")
```

---

<a id="q13"></a>
### 13. With the code snippet, explain how Multilabel classification is different from multiclass multioutput classification? (10M)

**Multilabel Classification**:
In multilabel classification, each instance (example) can belong to multiple classes simultaneously. The target variable contains multiple labels for each instance.

- **Example**: For a movie recommendation system, a movie can belong to multiple genres like "Action", "Comedy", and "Romance" simultaneously.

**Multiclass Multioutput Classification**:
In multiclass multioutput classification, each instance has multiple target variables, and each target variable is treated as a separate multiclass classification problem.

- **Example**: A multi-output model can predict two different labels, such as "Color" (Red, Blue, Green) and "Size" (Small, Medium, Large) for a product. Each label is a separate classification task.

**Difference**:
- **Multilabel Classification**: One label can belong to multiple classes for a given instance.
- **Multiclass Multioutput**: Multiple independent classification tasks are performed, each with its own set of possible outcomes.

**Example Code** (Multilabel Classification using `sklearn`):
```
from sklearn.datasets import make_multilabel_classification
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Generate a multilabel dataset
X, y = make_multilabel_classification(n_samples=1000, n_features=20, n_classes=3, n_labels=2, random_state=42)

# Split data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the model (KNN in this case)
model = KNeighborsClassifier()

# Train the model
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)

# Evaluate accuracy
print(f"Accuracy: {accuracy_score(y_test, predictions)}")
```

**Example Code** (Multiclass Multioutput Classification using `sklearn`):
```
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Generate a multiclass multioutput dataset
X, y1 = make_classification(n_samples=1000, n_features=20, n_informative=10, random_state=42)
y2 = y1 * 2  # Creating another target variable as a separate classification task

# Split data into train and test
X_train, X_test, y1_train, y1_test, y2_train, y2_test = train_test_split(X, y1, y2, test_size=0.2, random_state=42)

# Initialize the multi-output model
multi_target_model = MultiOutputClassifier(RandomForestClassifier())

# Train the model
multi_target_model.fit(X_train, [y1_train, y2_train])

# Make predictions
y1_pred, y2_pred = multi_target_model.predict(X_test)

# Evaluate accuracy for both outputs
print(f"Accuracy for y1: {accuracy_score(y1_test, y1_pred)}")
print(f"Accuracy for y2: {accuracy_score(y2_test, y2_pred)}")
```
---

<a id="q14"></a>
### 14. List a few of the typical performance measures for regression problems. (5M)

**Performance Measures for Regression Problems**:

1. **Mean Absolute Error (MAE)**:
   - Measures the average of the absolute errors between predicted and actual values.
   - Formula: `MAE = (1/n) * Σ|y_i - ŷ_i|`
   - A lower value indicates better performance.

2. **Mean Squared Error (MSE)**:
   - Measures the average of the squared differences between the predicted and actual values.
   - Formula: `MSE = (1/n) * Σ(y_i - ŷ_i)²`
   - A lower value indicates better performance.

3. **Root Mean Squared Error (RMSE)**:
   - The square root of MSE, provides error values in the same unit as the target variable.
   - Formula: `RMSE = sqrt((1/n) * Σ(y_i - ŷ_i)²)`
   - A lower value indicates better performance.

4. **R-squared (R²)**:
   - Measures the proportion of the variance in the dependent variable that is predictable from the independent variables.
   - Formula: `R² = 1 - Σ(y_i - ŷ_i)² / Σ(y_i - ȳ)²`
   - A higher value indicates better performance.

5. **Adjusted R-squared**:
   - Adjusts R² for the number of predictors in the model, penalizing for unnecessary variables.
   - Formula: `R²_adj = 1 - ((1 - R²) * (n - 1)) / (n - p - 1)`
   - A higher value indicates better performance when comparing models with different numbers of predictors.

---
